{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "f0bVPYf-2uAs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "R0NYR_VYmcjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to a journey through the world of Artificial Intelligence (AI), a field that no longer resides in the realms of science fiction but is a tangible part of our everyday lives. This project is crafted with a vision that anyone, regardless of their background, can gain valuable insights and knowledge. For those aspiring to enter the AI industry and carve out a career, understanding the mathematical concepts presented here is crucial. These foundations are not just academic; they are the bedrock upon which the field of AI is built.\n",
        "\n",
        "However, it is equally important to recognize that the complexities of machine learning and its dense mathematical underpinnings can be daunting. For individuals without prior exposure to machine learning, it might be tempting to sidestep these rigorous details. Yet, this guide is designed in such a way that even if you choose to bypass the intricate math, you can still significantly enhance your understanding of AI.\n",
        "\n",
        "In today's world, AI tools like GPT-4 are at everyone's disposal, serving as personal assistants, creative muses, and problem solvers. By improving your understanding of AI, you'll be better equipped to interact with these tools effectively. Whether it's crafting precise prompts or providing ChatGPT with clearer instructions, your enriched knowledge will translate into more powerful and beneficial interactions with AI in your daily life. Let's embark on this educational adventure together.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ge-qhuWQigjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful Links"
      ],
      "metadata": {
        "id": "zcB7ekl8no1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For readers interested in expanding their knowledge and exploring Generative AI further, the following resources can be incredibly helpful. These links offer a range of materials, from introductory courses to more advanced topics, and practical tools for implementing AI solutions.\n",
        "\n",
        "1. **Generative AI for Everyone - Coursera**: This course is designed to introduce the concepts of Generative AI to a broad audience, no technical background required.\n",
        "   [Generative AI for Everyone](https://www.coursera.org/learn/generative-ai-for-everyone)\n",
        "\n",
        "2. **Neural Networks and Deep Learning - Coursera**: For those seeking a deeper understanding of the underpinnings of AI, this course provides a solid foundation in neural networks and deep learning.\n",
        "   [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/)\n",
        "\n",
        "3. **OpenAI Platform Documentation**: An excellent starting point for anyone looking to get familiar with the tools and APIs offered by OpenAI.\n",
        "   [OpenAI Platform Documentation](https://platform.openai.com/docs/introduction)\n",
        "\n",
        "4. **OpenAI Cookbook on GitHub**: A repository of practical implementations and code examples for using OpenAI's technologies.\n",
        "   [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
        "\n",
        "5. **Langchain Python Documentation**: For those interested in implementing language AI with Python, Langchain's documentation is a great place to start.\n",
        "   [Langchain Python Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
        "\n",
        "These resources are valuable for both theoretical understanding and practical application. They are recommended as complementary materials to the content of this guide.\n"
      ],
      "metadata": {
        "id": "hPmEjPL3nnhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Learning Resources from DeepLearning.AI\n",
        "\n",
        "For those looking to further their understanding and skills in working with GPT and AI, DeepLearning.AI offers several short courses. You can check at here: https://www.deeplearning.ai/short-courses/ Here are some courses which are especially useful for developers and anyone interested in practical applications:\n",
        "\n",
        "1. **ChatGPT Prompt Engineering for Developers**: This course focuses on optimizing interactions with GPT models through effective prompt design.\n",
        "   [Course Link](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n",
        "\n",
        "2. **Building Systems with the ChatGPT API**: Learn how to integrate ChatGPT into various systems and applications.\n",
        "   [Course Link](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n",
        "\n",
        "3. **LangChain for LLM Application Development**: A course on using LangChain for developing applications with Large Language Models.\n",
        "   [Course Link](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n"
      ],
      "metadata": {
        "id": "LECYbsEjxzra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author's Disclaimer"
      ],
      "metadata": {
        "id": "Kg9ZrO5hmgbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the author of this guide, I am on a learning journey myself. I acknowledge that there may be instances where my explanations are not entirely accurate or even incorrect, and for this, I extend my sincerest apologies. In pursuit of rigor, I will provide links to relevant research papers when possible. However, it is essential to note that these references are merely the tip of the iceberg in the vast ocean of scholarly articles. The field of AI is burgeoning, with countless new papers—both academic and empirical—published daily. Readers are encouraged to delve deeper into the subject matter to further their understanding.\n",
        "\n",
        "It's important to mention that the content of this project is based on Colab Markdown, which may result in some formatting limitations. Additionally, the information provided will be subject to updates as the field evolves and as my own understanding deepens. I welcome discussions and inquiries from readers. Should you have any questions or wish to engage in a dialogue about the content, please feel free to contact me at: wyuxiang0228@gmail.com.\n",
        "\n",
        "Let us approach this exploration with open minds and a collaborative spirit. Together, we can navigate the complexities of AI and contribute to its growth and understanding."
      ],
      "metadata": {
        "id": "yEgLFBRnmG6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: What is Generative AI?"
      ],
      "metadata": {
        "id": "ltshPc4jfBP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI, in simple terms, refers to a type of large-scale Natural Language Processing (NLP) model. It functions by predicting subsequent text, thereby acting as an approximated logical or intelligent machine. While there are various perspectives on this topic, it's essential to understand the fundamental principle underlying current generative AI, exemplified by models like GPT-4. This principle is fundamentally about predicting the probability of the next word based on a given set of preceding words.\n",
        "\n",
        "Although there are attempts from a biomimetic perspective to draw parallels between these large models, constructed with stacked transformer architectures, and biological neural networks, the interpretability of these AI systems remains weak. It's crucial to emphasize that AI should never be regarded as a black box that can do anything, but rather as just a NN system.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Uou2SMtcnDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glossary of Terms in Generative AI"
      ],
      "metadata": {
        "id": "ALQDlZFsn2Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **NLP (Natural Language Processing):** A field of AI that focuses on the interaction between computers and human language, particularly how to program computers to process and analyze large amounts of natural language data.\n",
        "\n",
        "- **AI (Artificial Intelligence):** A field of computer science dedicated to creating systems capable of performing tasks that typically require human intelligence. These tasks include problem-solving, recognizing patterns, understanding language, and decision making.\n",
        "\n",
        "- **GenAI (Generative Artificial Intelligence):** A branch of AI focused on creating models that can generate new content or data that is similar to the data they were trained on. This includes text, images, music, and more.\n",
        "\n",
        "\n",
        "- **Transformer:** A type of neural network architecture that has become foundational in the field of natural language processing due to its effectiveness in handling sequential data, like text. It's known for its attention mechanism, which allows it to focus on different parts of the input sequence when making predictions.\n",
        "\n",
        "- **Attention:** A mechanism in AI, particularly in Transformer models, that allows the model to focus on different parts of the input data, thereby understanding context and relationships in the data more effectively.\n",
        "\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers):** A Transformer-based machine learning technique for natural language processing pre-training. It's designed to understand the context of a word based on all of its surroundings (left and right of the word).\n",
        "\n",
        "- **GPT (Generative Pre-trained Transformer):** A series of language models developed by OpenAI that use deep learning to produce human-like text. These models are pre-trained on a large corpus of text and then fine-tuned for specific tasks.\n",
        "\n",
        "- **LLM (Large Language Model):** A type of AI model designed to understand, interpret, and generate human language. These models are 'large' due to their extensive training data and the high number of parameters they contain.\n",
        "\n",
        "- **Token:** In NLP, a token is a sequence of characters that is treated as a single unit for the purpose of analysis. A token could be a word, a sentence, a punctuation mark, etc.\n",
        "\n",
        "- **Prompt Engineering:** The art of crafting inputs (prompts) to an AI model to elicit the desired output. It is especially important in generative AI, where the output is highly dependent on the input prompt.\n",
        "\n",
        "- **Fine-tuning:** A process in machine learning where a pre-trained model is further trained on a smaller, specific dataset to adapt it for a particular task.\n",
        "\n",
        "- **Pre-training:** The process of training a machine learning model on a large dataset before it is fine-tuned on a task-specific dataset. This gives the model a broad understanding of the language or task domain.\n",
        "\n",
        "- **RLHF (Reinforcement Learning from Human Feedback):** A training approach where a model is fine-tuned based on feedback from human interactions, enabling it to align more closely with human values and preferences.\n",
        "\n",
        "- **RAG (Retrieval-Augmented Generation):** A technique in NLP that combines a retrieval-based component (which fetches relevant documents or information) with a generative component (which generates text based on that information).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "znoMVAs5eCAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention in AI and Transformers (Optional)"
      ],
      "metadata": {
        "id": "QxLmYPrnn7qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In artificial intelligence, the concept of 'Attention' is a mechanism that helps models to focus on specific parts of the input data, which is particularly useful in tasks involving sequences, like language processing. The Transformer architecture, which utilizes this attention mechanism, has revolutionized the field of natural language processing.\n",
        "\n",
        "The Transformer model processes inputs (like words in a sentence) in parallel, rather than sequentially, which is a significant departure from previous sequence-to-sequence models. The key innovation is the attention mechanism, which allows the model to weigh the importance of different parts of the input data. Essentially, it 'pays attention' to specific elements that are more relevant to the task at hand.\n",
        "\n",
        "### Mathematical Formulation of the Attention Mechanism\n",
        "\n",
        "1. **Query, Key, and Value Vectors**:\n",
        "   $$ \\text{Query vector: } \\mathbf{Q} $$\n",
        "   $$ \\text{Key vector: } \\mathbf{K} $$\n",
        "   $$ \\text{Value vector: } \\mathbf{V} $$\n",
        "\n",
        "2. **Scoring Function**:\n",
        "   The score \\( S \\) is computed as a function of Query \\( \\mathbf{Q} \\) and Key \\( \\mathbf{K} \\) vectors.\n",
        "   $$ S(\\mathbf{Q}, \\mathbf{K}) = \\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\sqrt{d_k}} $$\n",
        "   Here, \\( d_k \\) is the dimension of the key vector, and the division by \\( \\sqrt{d_k} \\) is for scaling.\n",
        "\n",
        "3. **Softmax Layer**:\n",
        "   The scores are normalized using the Softmax function to ensure they sum up to one.\n",
        "   $$ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\cdot \\mathbf{V} $$\n",
        "\n",
        "The attention mechanism thus calculates a weighted sum of the Value vectors, with weights determined by the normalized scores, allowing the model to focus on more relevant parts of the input.This process enables the model to dynamically focus on different parts of the sentence, leading to a more nuanced understanding and generation of language.\n",
        "\n",
        "[Link to the Transformer Paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n"
      ],
      "metadata": {
        "id": "mH-6ShDoezlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2: Engaging with AI through ChatGPT"
      ],
      "metadata": {
        "id": "gQfdJQ_WoAs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting with ChatGPT"
      ],
      "metadata": {
        "id": "5T1gx0Qqp79Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT serves as an excellent starting point for individuals to engage with artificial intelligence. It's cost-effective, requiring only $20 per month, making it an accessible tool for anyone curious about AI. I highly recommend a subscription to everyone interested in this field.\n",
        "\n",
        "We will primarily be interacting with the GPT-4 model engine. While it may be expensive on a production level, it is incredibly valuable for personal use or for those who are experimenting with AI for the first time. The interaction with GPT is based on a chat-based interface, where the AI emulates a conversational partner.\n",
        "\n",
        "## The Importance of Clear Prompts\n",
        "\n",
        "The clarity of our prompts (the input we give to AI) is crucial for the quality of the AI's responses. Just as in human conversation, unclear speech makes it difficult to provide the desired response. The same principle applies to AI. To receive high-quality responses from GPT, we must provide clear and concise instructions.\n",
        "\n",
        "## Understanding GPT's Limitations\n",
        "\n",
        "It's essential to recognize that GPT is not an omnipotent 'black box' AI. Even with the current state-of-the-art GPT-4, its capabilities are still bounded. Therefore, we should temper our expectations for complex tasks. If the responses are unsatisfactory, we should try to provide more context and clearer directives. Moreover, breaking down tasks into smaller, manageable pieces can lead to better outcomes for two reasons:\n",
        "\n",
        "1. **Complexity and Description**: If a task is complex, our description tends to be complex as well. GPT might not capture every detail due to its limited 'memory length' or token window, similar to how the attention mechanism may not capture all pertinent information.\n",
        "\n",
        "2. **Task Difficulty**: Given the limited intelligence of GPT compared to human cognition, it may not be equipped to handle highly complex tasks effectively.\n",
        "\n",
        "In conclusion, while GPT-4 is a powerful tool, we must engage with it strategically, providing clear prompts, and breaking down complex tasks to leverage its capabilities fully."
      ],
      "metadata": {
        "id": "i0dmjuxBqCTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Case Study: Resume Editing\n",
        "\n",
        "Using GPT to enhance your resume can be a highly effective strategy if approached correctly. However, expecting GPT to produce a perfect resume in a single interaction is unrealistic, just as it would be for even top career professionals.\n",
        "\n",
        "### Incorrect Approach:\n",
        "\n",
        "- **Example of a Wrong Prompt**: \"Please help me improve my resume,\" followed by pasting the entire resume text.\n",
        "  - In this scenario, if you expect GPT to output a perfect resume, it's better to abandon that expectation. No service can provide a perfect outcome from a single interaction.\n",
        "\n",
        "### Correct Approach:\n",
        "\n",
        "- **Step 0**: Begin by attaching a job description (JD) and state that you wish to tailor your resume to this JD. Specify any resume conventions you want to follow (e.g., starting with action verbs).\n",
        "  \n",
        "- **Step 1**: Attach the current version of your resume. This step is crucial as it gives GPT context about your background. Explain that you want to go through each bullet point individually.\n",
        "\n",
        "- **Step 2**: Present each bullet point separately and ask GPT for suggestions on how to improve it. Engage in a discussion with GPT about that specific bullet point. If the response is not satisfactory, provide specific reasons why.\n",
        "\n",
        "- **Step 3**: After refining all bullet points, present both the JD and the revised resume to GPT for an evaluation and further advice. Based on the feedback, return to Step 2 for targeted revisions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "89YvnLGsrlP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Suggestions from My Personal Experience\n",
        "\n",
        "To optimize your use of GPT-4, I recommend adhering to the following guidelines based on my own experiences:\n",
        "\n",
        "1. **Use English**: Even if your English spelling or grammar isn't perfect, try to avoid using native language rather than English. The rationale is that GPT-4's training data predominantly comes from online content, which is primarily in English. Consequently, GPT-4 tends to be more proficient in English than other languages. Similarly, for coding questions, Python is widely used and well-represented on GitHub, which means GPT-4 is likely to be more effective with Python than less common programming languages.\n",
        "\n",
        "2. **Decompose Complex Tasks**: Break down complex tasks into simpler ones, as previously mentioned.\n",
        "\n",
        "3. **Use Delimiters and Section Titles**: For example, when providing your resume and job description, you could format it as follows:\n",
        "\n",
        "\n",
        "```python\n",
        "text = \"\"\" I will provide my resume and the JD, separated by '###'.\n",
        "\n",
        "### Resume ###\n",
        "[Your resume text]\n",
        "\n",
        "### JD ###\n",
        "[Job description text]\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "4. **Provide Your Insights**: GPT-4 is not merely a question-answering bot but an interactive one. Whenever possible, offer your own thoughts and reasoning. This helps GPT-4 provide responses that are more aligned with your perspective and understanding."
      ],
      "metadata": {
        "id": "fPvn8ySwvR2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, for an in-depth understanding of GPT-4's capabilities, you may refer to the following academic paper available on arXiv:\n",
        "\n",
        "- **\"Sparks of Artificial General Intelligence: Early experiments with GPT-4\"**: This paper, available in full via arXiv, presents detailed experiments and discussions on the capabilities of GPT-4, exploring its potential in simulating aspects of general intelligence.\n",
        "  [Read the full paper](https://arxiv.org/pdf/2303.12712.pdf)\n",
        "\n",
        "\n",
        "  For a deeper exploration into the method of breaking down complex problems into manageable sequences, consider the paper titled: （Optional）\n",
        "\n",
        "- **\"Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation\"** by Ruomeng Ding et al. This paper discusses how advancements in Large Language Models (LLMs) aid in decision-making by deconstructing intricate issues into language sequences or \"thoughts\". The paper introduces a novel approach termed \"Everything of Thoughts\" (XoT), which combines pretrained reinforcement learning with Monte Carlo Tree Search (MCTS) to enhance LLMs' problem-solving abilities. The findings show XoT's significant performance improvements in multi-solution tasks.\n",
        "  [Read the full paper](https://arxiv.org/abs/2311.04254)\n"
      ],
      "metadata": {
        "id": "PqmaJyJRtZy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 3: Understanding Model Architecture in Large Language Models (LLMs)"
      ],
      "metadata": {
        "id": "mnVpNVt-6o0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Large Language Models (LLMs) are at the forefront of AI advancements. They are complex neural networks with vast numbers of parameters, trained on extensive datasets using advanced methodologies.\n",
        "\n",
        "## Transformer Variants in LLMs\n",
        "The backbone of LLMs is the Transformer architecture, with three main variants:\n",
        "\n",
        "1. **Encoder+Decoder**: Models like T5 use a full Transformer structure, with separate stacks of encoders and decoders, suitable for a wide range of tasks.\n",
        "2. **Encoder-Only**: Models such as BERT consist only of the encoder portion of the Transformer, excelling in tasks like language understanding.\n",
        "3. **Decoder-Only**: Models like GPT fall under this category, where they are primarily used for generating text.\n",
        "\n",
        "## Focus on Decoder-Only Models\n",
        "Decoder-Only models, especially GPT, have become mainstream due to their effectiveness in text generation. They are capable of producing contextually relevant and coherent text.\n",
        "\n",
        "## T5 - An Encoder-Decoder Model\n",
        "T5, the \"Text-to-Text Transfer Transformer,\" utilizes the full Transformer model. It treats every task as a text-to-text problem, showcasing versatility across a range of tasks.\n",
        "\n",
        "## GPT - A Decoder-Only Model\n",
        "GPT models are prominent examples of Decoder-Only Transformers. They are highly effective for generating text and are used in various applications like content creation and text completion.\n",
        "\n",
        "## Conclusion\n",
        "Understanding these architectural variants is key to grasping how LLMs function. Each variant has its strengths, making them suitable for different AI applications."
      ],
      "metadata": {
        "id": "Qp3yAJz-6o22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Current LLMs Favor Decoder-Only Architecture （Optional）\n",
        "\n",
        "The architectural choice for current LLMs, particularly favoring the Decoder-Only model, stems from several key considerations:\n",
        "\n",
        "### 1. Better Zero-Shot Performance and Suitability for Large Unlabeled Datasets\n",
        "- **Early Comparisons**: Comparisons between encoder-decoder and decoder-only architectures in the era of smaller models highlighted specific strengths of each.\n",
        "- **Decisive Research**: A study by Google Brain and HuggingFace on 5B parameter models revealed that decoder-only models performed better in zero-shot scenarios, whereas encoder-decoder models required multitask finetuning on labeled data for optimal performance.\n",
        "- **Current Training Paradigms**: Large LMs are predominantly trained on vast unlabeled datasets, benefiting the zero-shot superiority of decoder-only models. The integration of techniques like RLHF further enhances this advantage.\n",
        "\n",
        "### 2. Emergent Abilities in Large-scale Data and Parameter-rich Models\n",
        "- **Beyond Traditional Scaling**: As model sizes increase, emergent abilities appear, breaking the traditional log-linear performance improvement pattern. This phenomenon has been observed in models that can perform complex reasoning and self-multitask finetuning.\n",
        "- **Encoder-Decoder Equivalence**: At larger scales, the inherent reasoning capabilities of LLMs can equate the benefits of multitask finetuning in encoder-decoder models.\n",
        "\n",
        "### 3. In-Context Learning as an Analog of Few-Shot Finetuning\n",
        "- **Practical Applications**: In-context information or Chain-of-Thought prompts in models like GPT enhance their potential for few-shot learning tasks.\n",
        "- **Research Insights**: Recent studies suggest that in-context information acts as a form of task finetuning, making decoder-only models more adept at learning from contextual cues.\n",
        "\n",
        "## Conclusion and Future Outlook\n",
        "\n",
        "Decoder-only architectures align more naturally with traditional language model patterns and are well-suited for open-domain tasks and models like ChatGPT. They excel in scenarios with limited labeled data and demonstrate strong performance in few-shot learning contexts. The future of LLMs and self-supervised training is likely to continue favoring the decoder-only architecture.\n"
      ],
      "metadata": {
        "id": "3Liaa_4l6o5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced In-Context Learning in Decoder-Only LLMs（Optional）\n",
        "\n",
        "In-context learning is like a dynamic fine-tuning that occurs within the Transformer architecture during prompt processing. It's suggested that in-context information acts as a micro-adjustment to the model's attention layers, similar to task-specific fine-tuning.\n",
        "\n",
        "Here is the mathematical representation of this process:\n",
        "\n",
        "$$\n",
        "\\hat{F}_{ICL}(q) = W_{ZSL}q + W_V X' (W_K X')^T q\n",
        "$$\n",
        "$$\n",
        "= W_{ZSL}q + \\text{LinearAttn}(W_V X', W_K X', q)\n",
        "$$\n",
        "$$\n",
        "= W_{ZSL}q + \\sum_i (W_V x_i' \\otimes (W_K x_i')^T) q\n",
        "$$\n",
        "$$\n",
        "= W_{ZSL}q + \\Delta W_{ICL} q\n",
        "$$\n",
        "$$\n",
        "= (W_{ZSL} + \\Delta W_{ICL}) q.\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $W_{ZSL}$ represents the weights learned during zero-shot learning.\n",
        "- $\\Delta W_{ICL}$ represents the in-context learning adjustment to the weights.\n",
        "- $q$ is the query vector from the prompt.\n",
        "\n",
        "\n",
        "This formulation shows how in-context prompts can modulate the attention mechanism, enabling the model to adjust its outputs based on the provided context, thereby enhancing its few-shot learning capabilities.\n",
        "\n",
        "For a comprehensive theoretical understanding of this concept, refer to the paper: \"Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers.\" [Read the Paper](https://arxiv.org/abs/2212.10559)\n"
      ],
      "metadata": {
        "id": "0WDWHCp26o79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Mastery of Prompt Engineering"
      ],
      "metadata": {
        "id": "vwFrOdSw_4i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## The Essence of Prompt Engineering\n",
        "\n",
        "Prompt engineering is the art of crafting input prompts to effectively interact with language models, such as GPT. It is a critical skill for developers working with Large Language Models and for everyday users engaging with ChatGPT.\n",
        "\n",
        "### Why Prompt Engineering is Vital\n",
        "\n",
        "- **Interactive Process**: Whether developing LLM applications or simply interacting with ChatGPT, prompt engineering is highly interactive. It requires iteration and refinement to discover the most effective prompts.\n",
        "- **Beware of Over-Simplification**: One should be wary of any claims suggesting there are \"10 best prompts\" that work universally. The effectiveness of a prompt is context-dependent and varies based on the specific interaction.\n",
        "\n",
        "### The Thoughtful Approach to Prompts\n",
        "\n",
        "- **Input and Output**: It is imperative to consider what you are inputting into the model and what you expect as the output. If the desired output is not achieved, the prompt needs to be revisited and revised accordingly.\n",
        "\n",
        "## Recommended Reading: Chain of Thought\n",
        "\n",
        "For a foundational understanding of prompt engineering, I highly recommend reading the seminal article on the \"Chain of Thought\" approach. It provides invaluable insights into how structured prompts can lead to better outputs from AI models.\n",
        "\n",
        "- **Chain of Thought Article**: A must-read for anyone looking to delve deeper into the nuances of prompt engineering.\n",
        "  [Read the Chain of Thought Article](https://arxiv.org/abs/2201.11903)\n",
        "\n",
        "## Further Learning Resources\n",
        "\n",
        "For those who wish to explore prompt engineering in greater detail, the following course is an excellent resource:\n",
        "\n",
        "- **ChatGPT Prompt Engineering for Developers**: This course from DeepLearning.AI is dedicated to teaching how to optimize interactions with GPT models through effective prompt design.\n",
        "  [Take the Course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n"
      ],
      "metadata": {
        "id": "rfYurWuO_0-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Advanced Techniques in AI - Fine Tuning and Quantization with a Focus on LoRa"
      ],
      "metadata": {
        "id": "ava5OMdrwRrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Fine-Tuning\n",
        "\n",
        "Fine-tuning is a critical process in the field of AI, especially in the realm of machine learning and deep learning models. It involves the adjustment of a pre-trained model to make it more suitable for a specific task or to adapt it to a new dataset. This process is essential because while pre-trained models offer a solid foundation, they may not be perfectly aligned with the nuances of every unique application.\n",
        "\n",
        "### Why Fine-Tuning?\n",
        "\n",
        "- **Model Customization**: Fine-tuning allows for the customization of a model to specific needs, enhancing its relevance and accuracy for particular tasks.\n",
        "- **Efficiency**: It's more efficient to fine-tune an existing model than to train a new model from scratch, especially for complex models like Large Language Models (LLMs).\n",
        "\n",
        "### Adapting to Specific Tasks and Datasets\n",
        "\n",
        "- **Task-Specific Adjustments**: By fine-tuning a model, we can adjust it to perform well on tasks it wasn't originally designed for, such as specific forms of text analysis or image recognition.\n",
        "- **Dataset Familiarity**: Fine-tuning also helps the model to better understand and interpret the characteristics of a new dataset, leading to improved performance.\n",
        "\n",
        "In summary, fine-tuning is a fundamental step in deploying AI models effectively, as it tailors a general model to specific requirements, thereby maximizing its potential.\n"
      ],
      "metadata": {
        "id": "TteN-MVSzQiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Concept of Quantization in AI\n",
        "\n",
        "Quantization in AI is a technique used to optimize models for better performance and efficiency. It plays a crucial role in making AI more accessible and practical, especially on devices with limited computational resources.\n",
        "\n",
        "### Role of Quantization\n",
        "\n",
        "- **Performance Enhancement**: By reducing the precision of the numbers used in computations, quantization can significantly speed up model inference.\n",
        "- **Efficiency Improvement**: Quantization decreases the memory footprint of models, making them lighter and faster to execute.\n",
        "\n",
        "### Making AI More Accessible\n",
        "\n",
        "- **Model Size Reduction**: By quantizing a model, its size can be reduced without a substantial loss in accuracy, facilitating deployment on platforms with storage constraints.\n",
        "- **Computational Requirements**: Lower precision calculations require less computational power, enabling the use of AI models on devices like mobile phones or embedded systems.\n",
        "\n",
        "Quantization, therefore, is a key step in democratizing AI, making powerful models usable across a wide range of devices and applications.\n"
      ],
      "metadata": {
        "id": "7rrjeUkCzbpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring LoRa (Low-Rank Adaptation)\n",
        "\n",
        "LoRa, or Low-Rank Adaptation, introduced by a team from Microsoft in 2021, has become a popular method for fine-tuning large language models, diffusion models, and other AI models.\n",
        "\n",
        "### Why LoRa?\n",
        "\n",
        "The motivation behind LoRa is to achieve high-quality fine-tuning results more efficiently. Traditional full-parameter fine-tuning is memory-intensive and requires high-performance GPUs. LoRa offers an efficient alternative that maintains the quality of the results and can be equivalent to full-parameter fine-tuning by adjusting the rank hyperparameter.\n",
        "\n",
        "### How Full-Parameter Fine-Tuning Works\n",
        "\n",
        "Full-parameter fine-tuning involves training all the parameters of a model, continuing the original pretraining but in a supervised manner with prompt/completion pairs. These parameters, or weights, are grouped into layers or modules in a neural network and represented as matrices.\n",
        "\n",
        "### The Mechanics of LoRa\n",
        "\n",
        "LoRa differs from full-parameter fine-tuning in two key ways:\n",
        "\n",
        "1. **Tracking Weight Changes**: Instead of updating weights directly, LoRa tracks changes to the weights.\n",
        "2. **Decomposing Weight Changes**: It decomposes the large matrix of weight changes into two smaller matrices containing the trainable parameters.\n",
        "\n",
        "This approach is based on the concept that multiplying two smaller matrices can produce a larger matrix, as exemplified by multiplying a 1x5 matrix with a 5x1 matrix to get a 5x5 matrix.\n",
        "\n",
        "LoRa provides a resource-efficient way to fine-tune large-scale models while maintaining high-quality outcomes, making it a valuable tool for AI practitioners.\n"
      ],
      "metadata": {
        "id": "_h-KyzC_zmNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more detailed information on LoRa fine-tuning and its practical applications, you can refer to the following resources:\n",
        "\n",
        "- **Entry Point AI Article on LoRa**: This article provides an easily understandable overview of LoRa fine-tuning, its mechanisms, and hyperparameters.\n",
        "  [Read the Entry Point AI Article](https://www.entrypointai.com/blog/what-is-lora-fine-tuning-how-it-works-and-the-hyperparameters/)\n",
        "\n",
        "- **Original LoRa Research Paper**: For an in-depth academic perspective, the original research paper by Microsoft introduces and discusses the LoRa method.\n",
        "  [Read the LoRa Research Paper](https://arxiv.org/abs/2106.09685)\n"
      ],
      "metadata": {
        "id": "sx_zBZ1w1Ab3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Q-LoRa\n",
        "\n",
        "Q-LoRa, a variant of the Low-Rank Adaptation (LoRa) method, extends the original LoRa concept by incorporating quantization techniques. This combination aims to further optimize the efficiency of fine-tuning large language models, particularly in terms of computational resources and memory usage. By integrating quantization into the LoRa framework, Q-LoRa seeks to balance model performance with resource constraints, making it a valuable technique for deploying AI models on a wider range of hardware, including devices with limited processing capabilities.\n",
        "\n",
        "For an in-depth understanding of Q-LoRa and its methodologies, you can refer to the following research paper:\n",
        "\n",
        "- **Q-LoRa Research Paper**: This paper provides a detailed exploration of the Q-LoRa approach, outlining its mechanisms, advantages, and potential applications in the realm of AI.\n",
        "  [Read the Q-LoRa Research Paper](https://arxiv.org/abs/2305.14314)\n"
      ],
      "metadata": {
        "id": "DwRmI61S1WHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Exploration: Pre-training and RLHF （Optional）"
      ],
      "metadata": {
        "id": "R1c_WP3x19bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this guide does not delve into the specifics of Pre-training and RLHF (Reinforcement Learning from Human Feedback), these are pivotal concepts in the field of AI. For readers with a keen interest in these areas, especially those with access to the necessary computational resources and expertise, the following research papers provide comprehensive insights:\n",
        "\n",
        "- **Pre-training Research Paper**: \"Pre-Trained Models: Past, Present and Future\" offers a comprehensive overview of the development and significance of pre-trained models in AI.\n",
        "  [Read the Pre-training Research Paper](https://ar5iv.org/abs/2106.07139)\n",
        "\n",
        "- **RLHF Research Paper**: The paper titled \"Deep Reinforcement Learning from Human Preferences\" delves into the nuances of using human feedback in reinforcement learning to align AI models with human values and preferences.\n",
        "  [Read the RLHF Research Paper](https://ar5iv.org/abs/1706.03741)"
      ],
      "metadata": {
        "id": "sL97DUxW2ed6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6: Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "f0bVPYf-2uAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Retrieval-Augmented Generation (RAG) is an advanced AI paradigm that enhances the capabilities of language models by combining the power of neural networks with external knowledge sources. This chapter provides an overview of RAG and its role in creating robust language models.\n"
      ],
      "metadata": {
        "id": "cLv6XrRID7wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Retrieval-Augmented Generation?\n",
        "\n",
        "RAG is a technique where a language model accesses an external database during inference to pull in relevant information. It is akin to an \"open-book\" exam for AI, allowing models to reference and integrate external data into their responses.\n"
      ],
      "metadata": {
        "id": "QyGgJAEmElIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Necessity of RAG\n",
        "\n",
        "While large language models have demonstrated remarkable abilities, they also have limitations:\n",
        "\n",
        "- **Difficulty with Long-Tail Memories**: They can't remember all knowledge from training, especially low-frequency, long-tail information.\n",
        "- **Obsolescence of Knowledge**: Knowledge within the parameters can quickly become outdated and hard to update without significant retraining costs and potential catastrophic forgetting.\n",
        "- **Computational Cost**: The sheer number of parameters makes training and inference computationally expensive.\n",
        "\n",
        "Humans, similarly, do not memorize all knowledge. Instead, we often look up less familiar and new information as needed. Language models, too, can be semi-parametric, utilizing an external non-parametric database to reference during inference. This paradigm is known as RAG or Retrieval-Augmented Language Model (RALM).\n",
        "\n"
      ],
      "metadata": {
        "id": "nzAxejc6Er_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How RAG Works\n",
        "\n",
        "RAG models use a query derived from the user input to retrieve relevant documents from a datastore, which the model then uses as a reference to generate the output. This approach helps to alleviate issues related to memory, outdated knowledge, and computational costs. It also provides a reliable source of information for the model's responses and prevents the leakage of private information from the model's weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "LT505ON6Evlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Robustness of RAG\n",
        "\n",
        "The robustness of Retrieval-Augmented Generation (RAG) models is a key concern in their real-world application. Just like humans, RAG models can be misled by incorrect information. If the content retrieved by the model is noisy, factually incorrect, or intentionally manipulated, the output can be significantly affected, leading to responses that are nonsensical or even harmful.\n",
        "\n",
        "### The Impact of Irrelevant or Misleading Information\n",
        "\n",
        "Consider the example from Wang et al.'s 2023 study, where a simple question about whether a German Shepherd could enter an airport — a scenario where ChatGPT would normally recognize the value of service dogs and respond affirmatively — was turned on its head due to misleading retrieved content. The retrieval module pulled a controversial statement from an encyclopedia entry about \"Old German Shepherds\" being a debated classification of dog breeds. As a result, the model's response was altered to incorrectly deny the dog entry to the airport, showcasing a drastic shift in stance induced by the flawed retrieval.\n",
        "\n",
        "### Citation for Further Reading\n",
        "\n",
        "For those interested in the detailed mechanics and implications of this phenomenon, Wang et al.'s paper is an essential read:\n",
        "\n",
        "- **Research Paper on RAG Robustness**: Wang, Yile, et al. \"Self-Knowledge Guided Retrieval Augmentation for Large Language Models.\" This paper discusses the impact of irrelevant or misleading information on RAG models and presents a framework for enhancing their robustness.\n",
        "  [Read the Paper](https://arxiv.org/abs/2310.05002)\n",
        "\n",
        "The example underscores the necessity for RAG models to have mechanisms that ensure the reliability and relevance of the information they retrieve. It highlights the ongoing need for research and development in creating RAG models that can discern and utilize information as wisely as possible.\n"
      ],
      "metadata": {
        "id": "VQjeDxeBEvp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addressing RAG's Challenges\n",
        "\n",
        "Several recent studies aim to enhance the robustness of models using RAG, tackling issues such as how to deal with incorrect information retrieval. Approaches range from self-knowledge guided retrieval to training models with noisy datasets to improve their resilience.\n"
      ],
      "metadata": {
        "id": "VuuN6HfsE264"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommended Studies on RAG\n",
        "\n",
        "For those interested in the robustness research on RAG, the following papers and resources are recommended:\n",
        "\n",
        "- **Self-Knowledge Guided Retrieval Augmentation for Large Language Models**: This paper explores a framework where models use self-knowledge to guide whether to use RAG for a given query.\n",
        "\n",
        "[Read the Paper](https://arxiv.org/abs/2310.05002)\n",
        "\n",
        "- **RECALL**: A benchmark focusing on language models' robustness against external counterfactual knowledge.\n",
        "\n",
        "[Read the Paper](https://arxiv.org/abs/2311.08147)\n",
        "\n",
        "- **Making Retrieval-Augmented Language Models Robust to Irrelevant Context**: An experimental analysis testing the use of NLI filtering and training with noisy contexts to enhance robustness.\n",
        "\n",
        "[Read the Paper](https://arxiv.org/abs/2310.01558)\n",
        "\n",
        "- **Chain-of-Note**: Enhancing robustness in retrieval-augmented language models by incorporating relevance notes and self-awareness in the generation process.\n",
        "\n",
        "[Read the Paper](https://arxiv.org/abs/2311.09210)\n",
        "\n",
        "- **Self-RAG**: A framework that enables models to decide when to retrieve, what to generate, and how to critique through self-reflection.\n",
        "\n",
        "[Read the Paper](https://arxiv.org/abs/2310.11511)\n",
        "\n",
        "Understanding RAG and its implications is pivotal for developing AI models that are not only intelligent but also reliable and up-to-date with the latest information.\n"
      ],
      "metadata": {
        "id": "fLHVxjobE4zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6 （Continued）:\n",
        "\n",
        "# Building a Conversational Knowledge Retrieval System"
      ],
      "metadata": {
        "id": "n4QjmDftG35m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Origin and Goals\n",
        "\n",
        "The project aims to develop a conversational knowledge retrieval system modeled after the work done at Guardian. The system will focus on extracting and providing information from selected articles on the VA (Veterans Affairs) website.\n",
        "\n",
        "## Approach: Utilizing RAG for Conversational Knowledge Retrieval\n",
        "\n",
        "RAG is an ideal starting point for this project, allowing the integration of retrieved information into a conversational AI system. The following sections will outline the steps and code required to build such a system.\n"
      ],
      "metadata": {
        "id": "zdixIKppHCzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Documentation\n",
        "\n",
        "## Data Source and Preprocessing\n",
        "\n",
        "The original data (knowledge) is sourced from the VA disability website, which encompasses an extensive collection of web pages. I have preemptively crawled these pages to form a JSON file for input into the large language model (LLM). While we could directly use URLs or save them as PDFs as the original input data, this would introduce additional web scraping steps into this notebook (which also requiring extremely similar web page structures) or the introduction of OCR (to recognize PDF content).\n",
        "\n",
        "## Knowledge Structure\n",
        "\n",
        "The knowledge base for the VA disability website is organized in a tree-like structure, which helps in systematically categorizing and retrieving information.\n",
        "\n",
        "- **Root Level**: The primary starting point is the main VA disability website.\n",
        "  - [VA Disability Website](https://www.va.gov/disability/)\n",
        "\n",
        "- **Branches**: From the root, several branches extend, representing specific categories or services provided by the VA. These include:\n",
        "  - [After You File a Claim](https://www.va.gov/disability/after-you-file-claim/)\n",
        "  - [Change of Address](https://www.va.gov/change-address/)\n",
        "  - [Change of Direct Deposit](https://www.va.gov/change-direct-deposit/)\n",
        "  - [Check Claim or Appeal Status](https://www.va.gov/claim-or-appeal-status/)\n",
        "  - [Decision Reviews](https://www.va.gov/decision-reviews/)\n",
        "  - [Dependency and Indemnity Compensation](https://www.va.gov/disability/dependency-indemnity-compensation/)\n",
        "  - [Exposure to Hazardous Materials](https://www.va.gov/disability/eligibility/hazardous-materials-exposure/)\n",
        "  - [Illnesses Within One Year of Discharge](https://www.va.gov/disability/eligibility/illnesses-within-one-year-of-discharge/)\n",
        "  - [PTSD Eligibility](https://www.va.gov/disability/eligibility/ptsd/)\n",
        "  - [Special Claims](https://www.va.gov/disability/eligibility/special-claims/)\n",
        "  - [Eligibility for Disability Benefits](https://www.va.gov/disability/eligibility/)\n",
        "  - [File a Disability Claim Online](https://www.va.gov/disability/file-disability-claim-form-21-526ez/)\n",
        "  - [Additional Forms for Filing a Claim](https://www.va.gov/disability/how-to-file-claim/additional-forms/)\n",
        "  - [Fully Developed Claims](https://www.va.gov/disability/how-to-file-claim/evidence-needed/fully-developed-claims/)\n",
        "  - [When to File Your Claim](https://www.va.gov/disability/how-to-file-claim/when-to-file/)\n",
        "  - [Evidence Needed for Disability Claim](https://www.va.gov/disability/how-to-file-claim/evidence-needed/)\n",
        "  - [How to File a Claim](https://www.va.gov/disability/how-to-file-claim/)\n",
        "  - [Upload Supporting Evidence](https://www.va.gov/disability/upload-supporting-evidence/)\n",
        "  - [View VA Payment History](https://www.va.gov/va-payment-history/)\n",
        "  - [View Your Disability Rating](https://www.va.gov/disability/view-disability-rating/)\n",
        "  \n",
        "\n",
        "- **Leaves**: The final and most detailed layer consists of the individual subtitles found within these webpages. They provide specific answers to questions related to each branch topic, forming a comprehensive knowledge base for users to explore.\n",
        "\n",
        "\n",
        "\n",
        "## Query Processing\n",
        "\n",
        "When a user query comes in, the first thing we need to determine is which web page is the most relevant. Here, we should avoid directly inputting this into the large language model considering scalability. If we have knowledge from thousands of web pages, it would be impractical to pass all of this to LLM at once, both in terms of token consumption and context window constraints. Hence, I initially use semantic search with Sentence BERT to decide which web pages are most related, selecting the top 3.\n",
        "\n",
        "## Contextual Understanding\n",
        "\n",
        "At this point, we have identified the top 3 directly related web pages. We need some more context to help LLM understand what these pages are actually about, so we introduce the web pages' introductions. An introduction is a content summary of the URL, which, due to the absence of relevant elements on some web pages, I have uniformly taken to be all text content from the main title up to the first subtitle.\n",
        "\n",
        "## Model Decision Making\n",
        "\n",
        "We save the titles and introductions in a dictionary form and pass them as part of the prompt to LLM to let it decide which are the most directly relevant. Subsequently, we will continue to search for secondary Q&A knowledge within the directly related content. Also, considering that these issues fundamentally belong to the same larger domain, we should also obtain 2 other related topics that the user can inquire about if interested.\n",
        "\n",
        "## Final Answer Review\n",
        "\n",
        "Thus, after the user's query is input, we will obtain a dictionary with the most directly related and two other relatively related titles. Then, the most relevant questions within the most relevant issue are determined in the same way through semantic search to obtain the top 3 most related content (subtitle-answer pairs) and pass them to the large language model as part of the prompt, along with titles and their URLs for 2 other related topics. After the large language model outputs, we use GPT-4 to decide whether this content is helpful. Of course, other models could also be used, or we could set some metrics manually and write a function to check if the requirements are met. At the same time, we could further improve how we handle cases that do not meet the satisfaction criteria, typically by increasing more context to produce sufficiently helpful content. Ultimately, what we achieve is a reviewed final answer.\n"
      ],
      "metadata": {
        "id": "3iDK7pA6HMsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o21ZRai6Hdfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "```"
      ],
      "metadata": {
        "id": "g3rUGzFRH5SD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# This is local llama2 quantized model(8bit)\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"llama-2-13b-chat.ggmlv3.q8_0.bin\",\n",
        "    temperature=0,\n",
        "    max_tokens=1500,\n",
        "    top_p=0.9,\n",
        "    n_ctx=2048\n",
        ")\n",
        "\n",
        "def retry_with_exponential_backoff(\n",
        "    func,\n",
        "    initial_delay: float = 1,\n",
        "    exponential_base: float = 1.5,\n",
        "    jitter: bool = True,\n",
        "    max_retries: int = 5,\n",
        "    errors: tuple = (openai.error.ServiceUnavailableError, openai.error.RateLimitError)\n",
        "):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        num_retries = 0\n",
        "        delay = initial_delay\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "\n",
        "            except errors as e:\n",
        "                num_retries += 1\n",
        "\n",
        "                if num_retries > max_retries:\n",
        "                    return {\n",
        "                        \"message\": \"Sorry, we are currently experiencing technical difficulties. Please try again later.\"\n",
        "                    }\n",
        "\n",
        "                delay *= exponential_base * (1 + jitter * random.random())\n",
        "                time.sleep(delay)\n",
        "\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "with open('scraped.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "USaFwen8IBNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TitleFinder Class Documentation\n",
        "\n",
        "## Class Overview\n",
        "\n",
        "`TitleFinder` is a Python class designed to facilitate the search for relevant titles within a dataset based on textual queries. It utilizes the `SentenceTransformer` library to generate embeddings for semantic comparison and integrates with OpenAI's GPT models via API or the Llama model to refine search results.\n",
        "\n",
        "### Initialization: `__init__(self, model_name, data_file, gpt_model_type=None, llama_model=None)`\n",
        "\n",
        "The constructor initializes the class with the following parameters:\n",
        "\n",
        "- `model_name`: Identifier for the pre-trained `SentenceTransformer` model.\n",
        "- `data_file`: File path to a JSON dataset containing titles and related content.\n",
        "- `gpt_model_type`: Optional model name for OpenAI's GPT. Can be gpt-3.5-turbo or gpt-4.\n",
        "- `llama_model`: Optional model reference for the Llama model.\n",
        "\n",
        "Upon instantiation, it:\n",
        "- Initializes the `SentenceTransformer` model.\n",
        "- Loads the dataset from the JSON file.\n",
        "- Precomputes embeddings for all titles in the dataset.\n",
        "- Retrieves the OpenAI API key from an environment variable (if using GPT).\n",
        "\n",
        "### Private Methods\n",
        "\n",
        "Private methods within `TitleFinder` are used internally by the class to perform specific tasks. These should not be called directly from outside the class.\n",
        "\n",
        "- `_find_similar_titles`: Finds the top `n` titles most similar to the query.\n",
        "- `_get_introductions_for_titles`: Retrieves introductions for a given list of titles.\n",
        "- `_ask_gpt_for_related_titles`: Queries OpenAI's GPT model to identify the most relevant titles based on the query and introductions.\n",
        "- `_ask_llama_for_related_titles`: Queries the Llama model to identify the most relevant titles based on the query and introductions.\n",
        "\n",
        "### Public Method: `find_related_title_dic(self, query)`\n",
        "\n",
        "This public method combines finding similar titles with querying either the GPT model or the Llama model. It takes a user query, identifies similar titles from the dataset, retrieves their introductions, and finally, it requests the chosen model to specify the most relevant titles and output a json format string.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "The `TitleFinder` class relies on the following libraries and resources:\n",
        "\n",
        "- `SentenceTransformer`: For generating semantic embeddings of text.\n",
        "- `json`: To load and parse the dataset file as well as handle JSON responses.\n",
        "- `numpy`: To process cosine similarity scores and perform sorting operations.\n",
        "- `openai`: The official OpenAI Python client library for API interaction (if using GPT).\n",
        "- `llama`: Required if using the Llama model function for title recommendation.\n",
        "    - `langchain`: A sub-dependency needed by the Llama model.\n",
        "    - `<binary_model_file>`: The binary model file required for the Llama function.\n",
        "\n",
        "\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The class includes error handling to validate the JSON response from the chosen model. It raises a `ValueError` if the response cannot be parsed as JSON.\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "```python\n",
        "# Instantiate the class with a model and data file\n",
        "finder = TitleFinder('all-MiniLM-L6-v2', 'scraped.json',gpt_model_type='gpt-4')\n",
        "\n",
        "# Example query for PTSD-related titles\n",
        "query = \"I have PTSD, what should I do\"\n",
        "\n",
        "# Find similar titles\n",
        "similar_titles = finder.find_related_title_dic(query)\n",
        "print(similar_titles)\n"
      ],
      "metadata": {
        "id": "VTBlX1X1IOBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "class TitleFinder:\n",
        "    def __init__(self, model_name, data_file, gpt_model_type=None, llama_model=None):\n",
        "        \n",
        "        # Initialize the sbert model with the given name\n",
        "        self.model_name = model_name\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        \n",
        "        # Initialize the LLM name with the given name (gpt-3.5-turbo or gpt-4 or llama)\n",
        "        self.gpt_model_type = gpt_model_type\n",
        "        self.llama_model = llama_model\n",
        "        \n",
        "        #load data from the specified file\n",
        "        with open(data_file, 'r', encoding='utf-8') as file:\n",
        "            self.data = json.load(file)\n",
        "            \n",
        "        # compute embeddings for all titles in the dataset for similarity search\n",
        "        self.title_embeddings = self.model.encode([item['title'] for item in self.data], convert_to_tensor=True)\n",
        "        \n",
        "        # Load the OpenAI API key from environment variables\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "    def find_related_title_dic(self, query):\n",
        "        # Find similar titles to the query and then ask LLM to identify the most related ones\n",
        "        similar_titles = self._find_similar_titles(query)\n",
        "        title_introduction_dict = self._get_introductions_for_titles(similar_titles)\n",
        "\n",
        "        # Check if both models are provided\n",
        "        if self.gpt_model_type and self.llama_model:\n",
        "            raise ValueError(\"Both GPT and Llama models are provided. Please specify only one model.\")\n",
        "        # Check if neither model is provided\n",
        "        elif not self.gpt_model_type and not self.llama_model:\n",
        "            raise ValueError(\"Neither GPT nor Llama model is provided. Please specify one model.\")\n",
        "        # If only GPT model is provided\n",
        "        elif self.gpt_model_type:\n",
        "            return self._ask_gpt_for_related_titles(query, title_introduction_dict)\n",
        "        # If only Llama model is provided\n",
        "        elif self.llama_model:\n",
        "            return self._ask_llama_for_related_titles(query, title_introduction_dict)\n",
        "\n",
        "\n",
        "    def _find_similar_titles(self, query, top_n=3):\n",
        "        # Encode the query to find the most similar titles using cosine similarity\n",
        "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
        "        cosine_scores = util.pytorch_cos_sim(query_embedding, self.title_embeddings)[0]\n",
        "        # Ensure cosine scores are in numpy array format for processing\n",
        "        if not isinstance(cosine_scores, np.ndarray):\n",
        "            cosine_scores = cosine_scores.cpu().numpy()\n",
        "        # Get indices of the top N most similar titles\n",
        "        top_results_indices = np.argsort(cosine_scores)[-top_n:][::-1]\n",
        "        return [self.data[idx]['title'] for idx in top_results_indices]\n",
        "\n",
        "    def _get_introductions_for_titles(self, similar_titles):\n",
        "        # Retrieve the introduction section for each similar title\n",
        "        introductions = {}\n",
        "        for title in similar_titles:\n",
        "            for item in self.data:\n",
        "                if item['title'] == title:\n",
        "                    introductions[title] = item['introduction']\n",
        "                    break\n",
        "        return introductions\n",
        "    \n",
        "    @retry_with_exponential_backoff\n",
        "    def _ask_gpt_for_related_titles(self, query, title_introduction_dict):\n",
        "        # Formulate a system message to prompt GPT to choose the most relevant titles\n",
        "        system_message = \"The following are titles and their introductions. Based on the user's query, identify the title that is directly related and two other titles that are most related. Provide your answer in the form of a JSON object with keys 'directly_related', 'related_1', and 'related_2'.\"\n",
        "        for title, introduction in title_introduction_dict.items():\n",
        "            system_message += f\"\\n\\nTitle: {title}\\nIntroduction: {introduction}\"\n",
        "        # Prepare the conversation for GPT-3 completion call\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": f\"My query is: {query}. Please respond in JSON format with the keys 'directly_related', 'related_1', and 'related_2' to indicate the most relevant titles.\"}\n",
        "        ]\n",
        "        # Call OpenAI's GPT and parse the response\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=self.gpt_model_type,\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "        response_text = completion['choices'][0]['message']['content']\n",
        "        try:\n",
        "            # Try to parse the JSON response from GPT\n",
        "            response_dict = json.loads(response_text)\n",
        "        except json.JSONDecodeError:\n",
        "            # Raise an error if the response is not valid JSON\n",
        "            raise ValueError(\"The response from the model is not valid JSON. Please check the system message.\")\n",
        "        return response_dict\n",
        "    \n",
        "    def _ask_llama_for_related_titles(self, query, title_introduction_dict):\n",
        "        # Formulate a system message for Llama in Lora style\n",
        "        title_introductions = \"\"\n",
        "        for title, introduction in title_introduction_dict.items():\n",
        "            title_introductions += f\"Title: {title}\\nIntroduction: {introduction}\\n\\n\"\n",
        "\n",
        "        instruction = (\"Based on the provided titles and their introductions, and the user's query,\"\n",
        "                       \" identify which title is directly related, and which two other titles are most related.\"\n",
        "                       \" Respond in a JSON format with keys 'directly_related', 'related_1', and 'related_2'.\")\n",
        "\n",
        "        input_context = f\"User's Query: {query}\\n\\n{title_introductions}\"\n",
        "\n",
        "        prompt_template = '''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {instruction}\n",
        "\n",
        "    ### Input:\n",
        "    {input}\n",
        "\n",
        "    ### Response:\n",
        "    '''\n",
        "\n",
        "        # Using the \"llama\" function directly\n",
        "        prompt = prompt_template.format(instruction=instruction, input=input_context)\n",
        "        response_text = llama(prompt)\n",
        "        #print(prompt)\n",
        "        try:\n",
        "            # Try to parse the JSON response from Llama\n",
        "            response_dict = json.loads(response_text)\n",
        "        except json.JSONDecodeError:\n",
        "            # Raise an error if the response is not valid JSON\n",
        "            raise ValueError(\"The response from the Llama model is not valid JSON. Please check the system message.\")\n",
        "\n",
        "        return response_dict\n",
        "\n",
        "\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "xJOOcSDWIUVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ContentProcessor Class Documentation\n",
        "\n",
        "## Class Overview\n",
        "\n",
        "`ContentProcessor` is a Python class that acts as a pipeline for processing user queries by utilizing the functionality of the `TitleFinder` class and further refining the results using OpenAI's GPT models or Llama. It streamlines the process of obtaining relevant content based on a user's query.\n",
        "\n",
        "### Initialization: `__init__(self, title_finder, gpt_model_type=None, llama_model=None):`\n",
        "\n",
        "The constructor initializes the class with the following parameters:\n",
        "\n",
        "- `title_finder`: An instance of `TitleFinder` class.\n",
        "- `gpt_model_type`: Optional model name for OpenAI's GPT. Can be gpt-3.5-turbo or gpt-4.\n",
        "- `llama_model`: Optional model reference for the Llama model.\n",
        "\n",
        "\n",
        "### Public Method: `process_query(self, query)`\n",
        "\n",
        "The main method provided for external use. It accepts a `query` string and returns the final answer after processing the query through various private methods.\n",
        "\n",
        "### Private Methods\n",
        "\n",
        "The class contains several private methods used internally to process the query.\n",
        "\n",
        "- `_get_related_content(self, related_titles_dict, data)`: Retrieves full content based on related titles provided by `TitleFinder`.\n",
        "- `_find_most_related_subtitles(self, query, directly_related_content, top_n=3)`: Identifies and extracts the most relevant subtitles and their corresponding content.\n",
        "- `_get_url_from_title(self, title)`: Finds the URL associated with a given title.\n",
        "- `_format_titles_and_urls(self, related_titles_response)`: Formats titles and URLs for Markdown presentation.\n",
        "- `_format_related_subtitles(self, related_subtitles)`: Formats the subtitles section for Markdown presentation.\n",
        "- `_generate_system_message_prompt(self, related_subtitles, related_titles_response)`: Generates a system message to instruct the GPT model on how to format the response.\n",
        "- `_ask_gpt_for_related_content(self, related_subtitles, related_titles_response, query)`: Sends the content and user query to the GPT model and retrieves a structured answer.\n",
        "- `_ask_llama_for_related_content(self, related_subtitles, related_titles_response, query)`: Sends the content and user query to the llama model and retrieves a structured answer.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "This class relies on the following libraries:\n",
        "\n",
        "- `SentenceTransformer`: To create embeddings for subtitles for similarity comparison.\n",
        "- `numpy`: For numerical operations, particularly handling cosine similarity scores.\n",
        "- `openai`: The OpenAI Python client library to make API requests.\n",
        "- `llama`: Required if using the Llama model function for title recommendation.\n",
        "    - `langchain`: A sub-dependency needed by the Llama model.\n",
        "    - `<binary_model_file>`: The binary model file required for the Llama function.\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The class assumes that the `TitleFinder` and other dependencies are correctly handling their errors. It is designed to work with the output provided by these components without additional error checking.\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "```python\n",
        "# Assuming `title_finder` is an already instantiated TitleFinder object.\n",
        "content_processor = ContentProcessor(title_finder)\n",
        "\n",
        "# Process a user's query\n",
        "query = \"What benefits am I eligible for as a veteran?\"\n",
        "answer = content_processor.process_query(query)\n",
        "print(answer)\n",
        "```"
      ],
      "metadata": {
        "id": "lKcDm4JEIiwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "class ContentProcessor:\n",
        "    def __init__(self, title_finder, gpt_model_type=None, llama_model=None):\n",
        "        self.title_finder = title_finder\n",
        "        self.gpt_model_type = gpt_model_type\n",
        "        self.llama_model = llama_model\n",
        "\n",
        "    def process_query(self, query):\n",
        "        # Use TitleFinder to find related titles\n",
        "        related_titles_response = self.title_finder.find_related_title_dic(query)\n",
        "\n",
        "        # Extract full content for the directly_related topic\n",
        "        content_details = self._get_related_content(related_titles_response, self.title_finder.data)\n",
        "\n",
        "        # Find the most related subtitles and their answers\n",
        "        related_subtitles = self._find_most_related_subtitles(query, content_details['directly_related_content'])\n",
        "\n",
        "        # Check if both models are provided or neither\n",
        "        if self.gpt_model_type and self.llama_model:\n",
        "            raise ValueError(\"Both GPT and Llama models are provided. Please specify only one model.\")\n",
        "        elif not self.gpt_model_type and not self.llama_model:\n",
        "            raise ValueError(\"Neither GPT nor Llama model is provided. Please specify one model.\")\n",
        "        \n",
        "        # If only GPT model is provided\n",
        "        if self.gpt_model_type:\n",
        "            final_answer = self._ask_gpt_for_related_content(related_subtitles, related_titles_response, query)\n",
        "        # If only Llama model is provided\n",
        "        elif self.llama_model:\n",
        "            final_answer = self._ask_llama_for_related_content(related_subtitles, related_titles_response, query)\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    \n",
        "    def _get_related_content(self, related_titles_dict, data):\n",
        "        # Initialize the dictionary to hold results\n",
        "        results = {\n",
        "            'directly_related_content': {},\n",
        "            'related_1': {},\n",
        "            'related_2': {}\n",
        "        }\n",
        "\n",
        "        # Extract the full content for the directly_related topic\n",
        "        directly_related_title = related_titles_dict.get('directly_related')\n",
        "        for item in data:\n",
        "            if item['title'] == directly_related_title:\n",
        "                results['directly_related_content'] = {\n",
        "                    'url': item.get('url', 'URL not available'),\n",
        "                    'content': [\n",
        "                        {\n",
        "                            'subtitle': content_item.get('subtitle', 'Subtitle not available'),\n",
        "                            'answer': content_item.get('answer', 'Answer not available')\n",
        "                        } for content_item in item.get('content', [])\n",
        "                    ]\n",
        "                }\n",
        "                break  # Stop searching once we've found the directly related content\n",
        "\n",
        "        # For related_1 and related_2, we only need the title and URL\n",
        "        for key in ['related_1', 'related_2']:\n",
        "            related_title = related_titles_dict.get(key)\n",
        "            for item in data:\n",
        "                if item['title'] == related_title:\n",
        "                    results[key] = {\n",
        "                        'title': related_title,\n",
        "                        'url': item.get('url', 'URL not available')\n",
        "                    }\n",
        "                    break  # No need to look further once we've found the related item.\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _find_most_related_subtitles(self, query, directly_related_content, top_n=3):\n",
        "        # Load the Sentence BERT model using the model name from title_finder\n",
        "        model = SentenceTransformer(self.title_finder.model_name)\n",
        "\n",
        "        # Extract subtitles and encode them\n",
        "        subtitles = [item['subtitle'] for item in directly_related_content['content']]\n",
        "        subtitle_embeddings = model.encode(subtitles, convert_to_tensor=True)\n",
        "\n",
        "        # Encode the query\n",
        "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        cosine_scores = util.pytorch_cos_sim(query_embedding, subtitle_embeddings)[0]\n",
        "\n",
        "        # Convert to numpy array if it's not already\n",
        "        if not isinstance(cosine_scores, np.ndarray):\n",
        "            cosine_scores = cosine_scores.cpu().numpy()\n",
        "\n",
        "        # Find the top_n indices with the highest cosine similarity scores\n",
        "        top_results_indices = np.argsort(cosine_scores)[-top_n:][::-1]\n",
        "\n",
        "        # Get the most related subtitles and their answers\n",
        "        related_subtitles = []\n",
        "        for idx in top_results_indices:\n",
        "            related_subtitles.append(directly_related_content['content'][idx])\n",
        "\n",
        "        return related_subtitles\n",
        "    \n",
        "    \n",
        "    def _get_url_from_title(self, title):\n",
        "        for item in self.title_finder.data:\n",
        "            if item['title'] == title:\n",
        "                return item.get('url', 'URL not available')\n",
        "        return 'URL not available'\n",
        "\n",
        "\n",
        "    def _format_titles_and_urls(self, related_titles_response):\n",
        "        # Format the titles and URLs section\n",
        "        titles_and_urls = (\n",
        "            f\"### Directly related website:\\n[{related_titles_response['directly_related']}]\" +\n",
        "            f\"({self._get_url_from_title(related_titles_response['directly_related'])})\\n\\n\" +\n",
        "            f\"### Other related website1:\\n[{related_titles_response['related_1']}]\" +\n",
        "            f\"({self._get_url_from_title(related_titles_response['related_1'])})\\n\\n\" +\n",
        "            f\"### Other related website2:\\n[{related_titles_response['related_2']}]\" +\n",
        "            f\"({self._get_url_from_title(related_titles_response['related_2'])})\\n\\n\"\n",
        "        )\n",
        "        return titles_and_urls\n",
        "\n",
        "    def _format_related_subtitles(self, related_subtitles):\n",
        "        # Format the subtitles section\n",
        "        subtitles_section = ''\n",
        "        for item in related_subtitles:\n",
        "            subtitles_section += f\"### {item['subtitle']}\\n{item['answer']}\\n\\n\"\n",
        "        return subtitles_section\n",
        "\n",
        "    def _generate_system_message_prompt(self, related_subtitles, related_titles_response):\n",
        "        # Section 1: Instructions\n",
        "        instructions = (\n",
        "            \"As a Veterans Affairs website assistant, you are provided necessary data to answer the user's query.\\n\"\n",
        "            \"Please follow these instructions for your response:\\n\"\n",
        "            \"1. Provide a **direct answer** to the user's query, referencing the directly related content following.\\n\"\n",
        "            \"2. Include **additional related topics** that might be beneficial to the user, with corresponding titles and URLs.\\n\"\n",
        "            \"3. Ensure your response is informative, using the data provided, and maintain a friendly and helpful tone.\\n\"\n",
        "            \"4. Structure your response using markdown formatting. Integrate titles as markdown headers and URLs as clickable links.\\n\"\n",
        "            \"5. Present the information in a concise and organized manner to facilitate user understanding.\\n\"\n",
        "            \"The contents are provided as following:\\n\"\n",
        "        )\n",
        "\n",
        "        # Section 2: Directly Related Content Titles and URLs\n",
        "        titles_and_urls = self._format_titles_and_urls(related_titles_response)\n",
        "\n",
        "        # Section 3: Directly Related Subtitles and contents\n",
        "        subtitles_section = self._format_related_subtitles(related_subtitles)\n",
        "\n",
        "        # Combine all sections into the system message\n",
        "        system_message = instructions + titles_and_urls + subtitles_section\n",
        "        return system_message\n",
        "    \n",
        "    @retry_with_exponential_backoff\n",
        "    def _ask_gpt_for_related_content(self, related_subtitles, related_titles_response, query):\n",
        "        # Generate the system message prompt\n",
        "        system_message = self._generate_system_message_prompt(related_subtitles, related_titles_response)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": f\"{query}.\"}\n",
        "        ]\n",
        "\n",
        "        # Call the OpenAI API with temperature set to 0 for consistent output\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=self.gpt_model_type,\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        # Extract and parse the response\n",
        "        response_text = completion.choices[0].message.content\n",
        "        #print(system_message)\n",
        "        return response_text\n",
        "\n",
        "    def _ask_llama_for_related_content(self, related_subtitles, related_titles_response, query):\n",
        "        # Generate the system message prompt using the same function used for GPT\n",
        "        system_message = self._generate_system_message_prompt(related_subtitles, related_titles_response)\n",
        "\n",
        "        # Construct the prompt following the specified format\n",
        "        llama_prompt = (\n",
        "            \"\\n\\nBelow is an instruction that describes a task, paired with further content to answer the user's query. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n\"\n",
        "            f\"{system_message}\\n\"\n",
        "            \"### Input:\\n\"\n",
        "            f\"User's Query: {query}\\n\\n\"\n",
        "            \"### Response:\\n\"\n",
        "        )\n",
        "        #print(llama_prompt)\n",
        "        # Pass the constructed prompt to the pre-defined LLaMA function\n",
        "        response = llama(llama_prompt)\n",
        "        \n",
        "        return response\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "5BMlBnPkIowG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ContentReview Class Documentation\n",
        "\n",
        "## Class Overview\n",
        "\n",
        "`ContentReview` is a Python class designed to iteratively refine answers to user queries. It leverages the `ContentProcessor` class to generate responses and reviews their helpfulness using OpenAI's most capable GPT-4 models for advanced reasoning. If an answer is not deemed helpful, the class modifies the search parameters and attempts to refine the answer.\n",
        "\n",
        "### Initialization: `__init__(self, content_processor)`\n",
        "\n",
        "The class is initialized with a `content_processor` object, enabling it to access content processing and GPT querying functionalities.\n",
        "\n",
        "### Public Method: `refine_answer(self, query, initial_top_n=3, max_iterations=10)`\n",
        "\n",
        "This method serves as the interface for refining answers. It accepts a `query` and optional parameters `initial_top_n` and `max_iterations` to control the refinement process. It returns the most accurate and helpful answer possible within the given constraints.\n",
        "\n",
        "### Private Method: `_review_answer_with_gpt(self, query, answer)`\n",
        "\n",
        "A method to internally review the helpfulness of an answer using OpenAI's GPT models. It submits the answer and the user's query to the model, asking if the provided answer is helpful.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "- `openai`: To make API requests to OpenAI's models for reviewing answers.\n",
        "- `ContentProcessor`: A prerequisite class used for initial processing of the user query.\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The class uses the responses from the OpenAI API to determine the next steps in the refinement process. It assumes that the `ContentProcessor` class and other dependencies are managing their errors effectively.\n",
        "\n",
        "## Environment Variables\n",
        "\n",
        "The class requires the OpenAI API key to be set in the environment for making API calls.\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "```python\n",
        "# Assuming `content_processor` is an already instantiated ContentProcessor object.\n",
        "content_review = ContentReview(content_processor)\n",
        "\n",
        "# Refine an answer to the user's query\n",
        "query = \"How do I update my address with the VA?\"\n",
        "refined_answer = content_review.refine_answer(query)\n",
        "print(refined_answer)\n"
      ],
      "metadata": {
        "id": "c9ftgcIeI4qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "class ContentReview:\n",
        "    def __init__(self, content_processor):\n",
        "        self.content_processor = content_processor\n",
        "    \n",
        "    def refine_answer(self, query, initial_top_n=3, max_iterations=10):\n",
        "        top_n = initial_top_n\n",
        "        for iteration in range(max_iterations):\n",
        "            # Use the ContentProcessor to get an answer\n",
        "            final_answer = self.content_processor.process_query(query)\n",
        "\n",
        "            # Review the answer's helpfulness\n",
        "            is_helpful = self._review_answer_with_gpt(query, final_answer)\n",
        "\n",
        "            if is_helpful:\n",
        "                # If the answer is helpful, return it\n",
        "                return final_answer\n",
        "            else:\n",
        "                # If not helpful, increase the scope of content and try again\n",
        "                top_n += 5\n",
        "                print(f\"Refining answer, iteration {iteration + 1}/{max_iterations}, using top {top_n} subtitles.\")\n",
        "\n",
        "        # If a satisfactory answer isn't found, return a message indicating so\n",
        "        return \"I'm sorry, I wasn't able to find a satisfactory answer to your question.\"\n",
        "    \n",
        "    @retry_with_exponential_backoff\n",
        "    def _review_answer_with_gpt(self, query, answer):\n",
        "        # Prepare the system message for GPT to review the answer\n",
        "        system_message = f\"Review the following answer for the query '{query}':\\n\\n{answer}\\n\\nIs this answer helpful for the query? Respond with 'yes' or 'no'.\"\n",
        "\n",
        "        # Prepare the messages in the same format as for the OpenAI API call\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": \"Please review the above answer.\"}\n",
        "        ]\n",
        "\n",
        "        # Call the OpenAI API with temperature set to 0 for consistent output\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        # Extract and parse the response\n",
        "        response_text = completion.choices[0].message.content.strip().lower()\n",
        "\n",
        "        # Determine if the answer is considered helpful by the model's response\n",
        "        is_helpful = response_text == \"yes\"\n",
        "        return is_helpful\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "-bgR2vebI58P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "# Initilization\n",
        "title_finder = TitleFinder('all-MiniLM-L6-v2', 'scraped.json',gpt_model_type='gpt-3.5-turbo')\n",
        "content_processor = ContentProcessor(title_finder,gpt_model_type='gpt-3.5-turbo')\n",
        "content_review = ContentReview(content_processor)\n",
        "\n",
        "query = \"what if i have PTSD?\"\n",
        "reviewed_answer = content_review.refine_answer(query)\n",
        "display(Markdown(reviewed_answer))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3l35xJBxJCKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflections and Prospects for Future Work\n",
        "\n",
        "One area ripe for improvement lies in the transition from web pages (or PDFs) to a structured knowledge base. Currently, I utilize web scraping followed by semantic search at each level to extract specific content segments. In my view, there are fundamentally **two approaches** to accomplish this: the method currently employed, and the alternative of using a **vector database**, like Pinecone. This database can retrieve the best chunks based on embedding similarity scores and then feed them into a large language model for processing. For instance, the Langchain framework, based on **ReAct**, performs knowledge retrieval Q&A from PDFs. However, I believe simplistic chunking introduces issues. Fixed chunk token windows inevitably lead to content fragmentation, and fundamentally, these knowledge pieces are interconnected, which prompted me to adopt a tree-like structure.\n",
        "\n",
        "If we set top n = 1, essentially, we're using the **Sentence BERT** model to navigate a tree-structured knowledge base, selecting the most suitable branches at each node. But as I mentioned earlier, the knowledge areas are interconnected, so I advise against a top n = 1 setting. After retrieval, a large language model understands the given content and addresses user queries, backed by a system to evaluate the helpfulness of responses. If an answer doesn't meet the set metrics, the context is expanded to improve it.\n",
        "\n",
        "\n",
        "## Future work may include:\n",
        "- **Defining Metrics**: This task requires detailed work and the input of domain experts for formulation or evaluation.\n",
        "- **Model Modification**: Presently, large language models are based on general text, and some fine-tuning might be necessary for this system's specific purposes.\n",
        "- **Ethical Compliance**: Given the sensitive nature of the content, the system must adhere to ethical standards. This includes confidentiality (e.g., clarifying disclaimers about the storage of Q&A data to users) and care (ensuring responses are beneficial, especially since the content is intended for disabled veterans).\n",
        "\n",
        "## Additional areas to consider:\n",
        "- **User Experience Design**: Streamlining how users interact with the knowledge base is crucial. Implementing feedback loops can contribute to continuous improvement.\n",
        "- **Accessibility Considerations**: With a focus on disabled veterans, ensuring system accessibility is paramount, possibly requiring voice navigation, screen reader compatibility, and other assistive technologies.\n",
        "- **Data Security and Privacy**: Technical measures for data security, such as encryption and stringent access controls, are essential.\n",
        "- **Continuous Learning**: A system where the model improves its accuracy and helpfulness through learning from interactions, while upholding ethical data usage.\n",
        "- **Collaboration with Subject Matter Experts**: Regular consultation with veterans' affairs experts and disabled veterans to ensure content relevance and sensitivity.\n",
        "- **Scalability**: Developing strategies for the system to handle increased data and queries without compromising performance.\n",
        "- **Multilingual Support**: Adding support for multiple languages to serve the diverse veteran population better.\n",
        "\n",
        "## Deployment Considerations\n",
        "\n",
        "Deployment strategies play a critical role in the project's success, particularly when considering cloud platforms like AWS. It's essential to factor in the costs since inference operations often require GPU resources, which can be expensive and consume significant computational power. The choice between using OpenAI's API or fine-tuning and deploying open-source models on the cloud can be determined by the user volume and a more specific project needs.\n",
        "\n",
        "- **Cost Management on AWS**: When deploying on AWS, one must be mindful of the cost associated with GPU usage for model inference. Efficient resource management and scaling strategies are imperative to control expenses.\n",
        "- **Inference Strategy**: Depending on the volume of user queries, a decision has to be made between using OpenAI's API, which offers ease of use but at a cost at each query, or fine-tuning and deploying open-source models such as LLaMa2 on the cloud, which could be more cost-effective on a larger scale but requires more setup and maintenance.\n",
        "- **Scalability**: The system must scale dynamically based on user demand, balancing cost with performance. This includes considering serverless architectures or container services like Kubernetes to handle variable loads efficiently.\n",
        "- **Performance Optimization**: Implementing caching mechanisms to answer extreme similar questions in a given time window and optimizing query processing can reduce the need for frequent model invocations, thereby saving on computational costs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t1d2U2v4JH1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: The Transformer Model"
      ],
      "metadata": {
        "id": "_oW3C1XRLuG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the Transformer model is crucial for delving into the world of Natural Language Processing (NLP) and Large Language Models (LLMs). This chapter offers a detailed exploration of the Transformer architecture, drawing from the original paper \"Attention is All You Need\" and renowned resources like Jay Alammar's \"The Illustrated Transformer.\"\n",
        "\n",
        "## The Emergence of the Transformer\n",
        "\n",
        "The Transformer model captures long-range dependencies in sequences and facilitates parallel computation, a significant leap from traditional RNN/LSTM models. The traditional sequential processing models suffered from limitations in capturing lengthy temporal dependencies and lacked parallel computation capabilities.\n",
        "\n",
        "Transformers broke from this tradition by adopting an entirely new Encoder-Decoder structure based on self-attention mechanisms.\n",
        "\n",
        "## Understanding Self-Attention\n",
        "\n",
        "Self-attention, a core component of the Transformer, allows the model to focus on different parts of the input sequence for better representation. For example, in the sentence \"The animal didn't cross the street because it was too tired,\" self-attention helps the model discern whether \"it\" refers to \"animal\" or \"street.\"\n",
        "\n",
        "### Self-Attention Mechanics\n",
        "\n",
        "1. **Vector Projection**: Each input word is transformed into three vectors: Query (Q), Key (K), and Value (V) through linear layers.\n",
        "2. **Score Calculation**: The score between the query vector and key vectors is calculated using dot products.\n",
        "3. **Scaling**: Scores are scaled down to stabilize the gradients during backpropagation.\n",
        "4. **Softmax Normalization**: Ensures that the output scores are positive and sum to 1.\n",
        "5. **Weighted Value Vectors**: The softmax scores are used to weigh the value vectors.\n",
        "6. **Summation**: The weighted value vectors are summed to produce the final self-attention output for each position.\n",
        "\n",
        "Through self-attention, the model effectively captures various aspects of contextual information with simple matrix transformations.\n",
        "\n",
        "## Multi-Head Attention\n",
        "\n",
        "Multi-head attention in Transformers is analogous to multiple convolutional filters in CNNs, capturing different facets of information in a text sequence. It performs self-attention in parallel, concatenating the results, and applying a linear transformation.\n",
        "\n",
        "## Transformer's Basic Structure\n",
        "\n",
        "### 4.1 Encoder\n",
        "\n",
        "The encoder in the Transformer processes each word through self-attention, followed by a residual connection and Layer Normalization, then through a feedforward network. The structure of the encoder is as follows:\n",
        "\n",
        "- **Residual Connections**: Facilitate gradient flow in deep networks.\n",
        "- **Layer Normalization**: Stabilizes the learning process and normalizes the outputs of each layer.\n",
        "- **Feedforward Network**: Enhances the representation of each word.\n",
        "\n",
        "### 4.2 Decoder\n",
        "\n",
        "The decoder in the Transformer has a similar structure to the encoder but includes an additional 'Encoder-Decoder attention' layer. This layer helps the decoder focus on relevant parts of the input sentence.\n",
        "\n",
        "### 4.3 Overall Architecture\n",
        "\n",
        "The Transformer's architecture combines multiple encoders and decoders, with embeddings added to the input and output tokens. Positional encodings are also integrated to provide sequence information to the model.\n",
        "\n",
        "## Training and Masking Techniques\n",
        "\n",
        "Transformers use a training method called 'teacher forcing' and implement attention masks to prevent 'peeking' at the correct answers during training. The model's output layer consists of a linear transformation followed by a softmax function to predict the probability of each token.\n",
        "\n",
        "By exploring the Transformer's design and functionality, this chapter sheds light on one of the most influential architectures in modern NLP. The Transformer's ability to process sequences in parallel and capture complex dependencies has paved the way for the development of advanced models like BERT and GPT."
      ],
      "metadata": {
        "id": "d695gs0RLw0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Understanding BERT"
      ],
      "metadata": {
        "id": "xJ-hbVN1KBMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT, short for Bidirectional Encoder Representations from Transformers, represents a significant milestone in the history of Natural Language Processing (NLP) since its introduction in 2018. This chapter is designed for readers with a basic understanding of transformers and the attention mechanism.\n",
        "\n",
        "## The Advent of BERT\n",
        "\n",
        "BERT revolutionized the field by using contextual predictions. Unlike context-free embedding models like word2vec, BERT generates word embeddings based on the context, providing different embeddings for the same word based on its usage.\n",
        "\n",
        "### Contextual Predictions\n",
        "\n",
        "For instance:\n",
        "\n",
        "- Sentence A: He got bit by Python.\n",
        "- Sentence B: Python is my favorite programming language.\n",
        "\n",
        "BERT assigns different embedding vectors to the word \"Python\" in these sentences. This process of connecting a word to the rest of the context is based on attention mechanisms.\n",
        "\n",
        "### General Purpose Pre-training\n",
        "\n",
        "The paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" introduces the idea of a universal language model that can be pre-trained on a corpus of text and then fine-tuned for specific applications. BERT's pre-training removes the need for task-specific architecture modifications, making it the first fine-tuning based representation model capable of learning representations across tasks.\n",
        "\n",
        "For example, to classify spam emails:\n",
        "\n",
        "```python\n",
        "# Pseudo-code showing a simple classification layer after BERT\n",
        "model = BERTModel()\n",
        "classification_layer = FullyConnectedLayer()\n",
        "output = classification_layer(model(input))\n",
        "```\n",
        "\n",
        "## BERT's Architecture\n",
        "\n",
        "BERT, utilizing the transformer architecture, specifically leverages its encoder side. This involves a self-attention mechanism that accounts for the context of each token, providing bidirectionality, a distinct advantage over unidirectional models like Bi-LSTM. There are two primary variants of BERT, differing in size and complexity:\n",
        "\n",
        "- **BERT Base**: Consisting of 12 layers (L=12), with each layer having a hidden size of 768 (H=768) and 12 attention heads (A=12). This results in a total of about 110 million parameters.\n",
        "- **BERT Large**: Features 24 layers (L=24), a hidden size of 1024 (H=1024), and 16 attention heads (A=16), summing up to a total of around 340 million parameters.\n",
        "\n",
        "## Pre-training BERT Models\n",
        "\n",
        "### 3.1 Preparing the Training Data: Input Representation\n",
        "\n",
        "The input representation in BERT is designed to clearly define single or pairs of text sentences within a token sequence. The structure includes:\n",
        "\n",
        "- **Token Embeddings**: Transforms each word into a fixed-size vector, specifically a 768-dimensional vector in BERT.\n",
        "- **Segment Embeddings**: Used to differentiate between the two sentences in a pair.\n",
        "- **Position Embeddings**: Since transformers do not inherently capture sequence order, position embeddings are used to encode this information.\n",
        "\n",
        "These three types of embeddings are summed element-wise to create the final vector representation for each token.\n",
        "\n",
        "### 3.2 Masked LM (MLM)\n",
        "\n",
        "In the MLM task, 15% of the tokens in each sentence are randomly masked, and the model is trained to predict the original vocabulary ID based on the context.\n",
        "\n",
        "### 3.3 Next Sentence Prediction (NSP)\n",
        "\n",
        "NSP is a task designed to improve the model's understanding of the relationship between two sentences, which is vital for tasks such as question answering (QA) and natural language inference (NLI).\n",
        "\n",
        "By comprehensively explaining BERT's architecture and its pre-training approach, this chapter provides an in-depth understanding of one of the most transformative models in NLP.\n",
        "\n"
      ],
      "metadata": {
        "id": "nME1ACbpKzi_"
      }
    }
  ]
}