{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee32e77-771b-4f7d-a62e-781d159393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from functools import wraps\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb7defc-4338-463f-8652-1f1a8aef6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b105972c-6042-4a9f-9709-724bf6071bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93697b6-2640-41fd-9135-08ca4965dfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda26c45f58a4797b31eeb6c7a59b119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with quantization and other configuration\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    token=\"##\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\"\n",
    ") #use device_map=\"auto\" for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e987f788-586d-4d15-b54b-914434ef4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def logging_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract the system and user prompts\n",
    "        system_prompt = kwargs.get('system_prompt') if 'system_prompt' in kwargs else args[0]\n",
    "        user_prompt = kwargs.get('user_prompt') if 'user_prompt' in kwargs else args[1]\n",
    "\n",
    "        # Calculate the total number of words in the input prompts\n",
    "        input_word_count = len(system_prompt.split()) + len(user_prompt.split())\n",
    "\n",
    "        # Execute the function and get the result\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        # Calculate the total number of words in the output\n",
    "        output_word_count = len(result.split())\n",
    "\n",
    "        # Log the results\n",
    "        logging.info(f\"Execution Time: {execution_time:.2f} seconds\")\n",
    "        logging.info(f\"Input Word Count: {input_word_count}\")\n",
    "        logging.info(f\"Output Word Count: {output_word_count}\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0af79f-bfe5-478f-aaa5-912427740b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@logging_decorator\n",
    "def generate_text(system_prompt, \n",
    "                  user_prompt, \n",
    "                  pipeline=text_generation_pipeline,\n",
    "                  print_prompt=False):\n",
    "    # Prepare the message prompts\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Generate the prompt\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Optionally print the entire prompt\n",
    "    if print_prompt:\n",
    "        print(\"Generated Prompt: \", prompt)\n",
    "\n",
    "    # Define the termination tokens\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # Perform the text generation\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=4000,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False, #set to false so every time output is same, also ignore top p and temperature args\n",
    "        pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Return the generated text\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83db9125-df82-481a-8705-d1d1e3ef703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You're a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bae6e6a-a7e1-44a2-b0a5-e62995492e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is the architecture of LLaMa-3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a44d49-7dc1-4b3d-b7ba-2cb99054657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 06:19:46,713 - INFO - Execution Time: 14.38 seconds\n",
      "2024-06-28 06:19:46,713 - INFO - Input Word Count: 10\n",
      "2024-06-28 06:19:46,714 - INFO - Output Word Count: 387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 is a large language model developed by Meta AI, and its architecture is based on the transformer model introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. Here's a high-level overview of the architecture:\n",
      "\n",
      "1. **Encoder**: The encoder is a stack of identical layers, each consisting of two sub-layers:\n",
      "\t* **Self-Attention Mechanism**: This mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It's composed of three linear layers (query, key, and value) and a softmax function.\n",
      "\t* **Feed-Forward Network (FFN)**: This is a fully connected feed-forward network with a ReLU activation function and a linear output layer.\n",
      "2. **Decoder**: The decoder is also a stack of identical layers, each consisting of three sub-layers:\n",
      "\t* **Self-Attention Mechanism**: Similar to the encoder, this mechanism allows the model to attend to different parts of the input sequence and weigh their importance.\n",
      "\t* **Encoder-Decoder Attention**: This mechanism allows the model to attend to the encoder output and weigh its importance.\n",
      "\t* **FFN**: Similar to the encoder, this is a fully connected feed-forward network with a ReLU activation function and a linear output layer.\n",
      "3. **Positional Encoding**: To help the model understand the position of each token in the input sequence, positional encoding is added to the input embeddings.\n",
      "4. **Input Embeddings**: The input text is first embedded into a vector space using a learned embedding layer.\n",
      "5. **Output Layer**: The final output of the decoder is a probability distribution over the vocabulary, which is used to generate the next token in the sequence.\n",
      "\n",
      "LLaMA-3 uses a combination of these components to generate text. The model is trained on a large corpus of text data and uses a masked language modeling objective to predict the missing tokens in the input sequence.\n",
      "\n",
      "Some specific details about LLaMA-3's architecture include:\n",
      "\n",
      "* The model has 12 encoder layers and 12 decoder layers.\n",
      "* The self-attention mechanism uses a combination of parallel and serial attention mechanisms.\n",
      "* The FFN layers use a ReLU activation function and a linear output layer.\n",
      "* The model uses a byte-level byte-pair encoding (BPE) tokenizer to split the input text into subwords.\n",
      "\n",
      "Keep in mind that this is a simplified overview, and there may be additional details or nuances in the actual architecture of LLaMA-3.\n"
     ]
    }
   ],
   "source": [
    "Answer = generate_text(system_prompt,user_prompt)\n",
    "\n",
    "print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa0cad8-fb61-4dfc-ae90-e9e8b4d03911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLaMA-3 is a large language model developed by Meta AI, and its architecture is based on the transformer model introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. Here's a high-level overview of the architecture:\n",
       "\n",
       "1. **Encoder**: The encoder is a stack of identical layers, each consisting of two sub-layers:\n",
       "\t* **Self-Attention Mechanism**: This mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It's composed of three linear layers (query, key, and value) and a softmax function.\n",
       "\t* **Feed-Forward Network (FFN)**: This is a fully connected feed-forward network with a ReLU activation function and a linear output layer.\n",
       "2. **Decoder**: The decoder is also a stack of identical layers, each consisting of three sub-layers:\n",
       "\t* **Self-Attention Mechanism**: Similar to the encoder, this mechanism allows the model to attend to different parts of the input sequence and weigh their importance.\n",
       "\t* **Encoder-Decoder Attention**: This mechanism allows the model to attend to the encoder output and weigh its importance.\n",
       "\t* **FFN**: Similar to the encoder, this is a fully connected feed-forward network with a ReLU activation function and a linear output layer.\n",
       "3. **Positional Encoding**: To help the model understand the position of each token in the input sequence, positional encoding is added to the input embeddings.\n",
       "4. **Input Embeddings**: The input text is first embedded into a vector space using a learned embedding layer.\n",
       "5. **Output Layer**: The final output of the decoder is a probability distribution over the vocabulary, which is used to generate the next token in the sequence.\n",
       "\n",
       "LLaMA-3 uses a combination of these components to generate text. The model is trained on a large corpus of text data and uses a masked language modeling objective to predict the missing tokens in the input sequence.\n",
       "\n",
       "Some specific details about LLaMA-3's architecture include:\n",
       "\n",
       "* The model has 12 encoder layers and 12 decoder layers.\n",
       "* The self-attention mechanism uses a combination of parallel and serial attention mechanisms.\n",
       "* The FFN layers use a ReLU activation function and a linear output layer.\n",
       "* The model uses a byte-level byte-pair encoding (BPE) tokenizer to split the input text into subwords.\n",
       "\n",
       "Keep in mind that this is a simplified overview, and there may be additional details or nuances in the actual architecture of LLaMA-3."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(Answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db8fae-6e3e-4586-a313-a842fdc856a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c13e11-9df7-4515-8900-896b45eb22b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
